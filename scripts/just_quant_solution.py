# scripts/just_quant_solution.py

import pandas as pd
import numpy as np
import re
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm
from copy import deepcopy
from pathlib import Path
import optuna
from optuna.samplers import TPESampler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import balanced_accuracy_score
from sklearn.utils import class_weight
from xgboost import XGBClassifier

# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ —ç—Ç–∏ –º–æ–¥—É–ª–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ src/
from src.feature_eng import generate_features
from src.labeling import three_barrier_std, barrier_std


class JustQuantSolution:
    def __init__(self, max_period=20):
        self.max_period = max_period
        self._setup_sentiment_model()

    def _setup_sentiment_model(self):
        print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–∞...")
        sentiment_model_name = "blanchefort/rubert-base-cased-sentiment"
        self.sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)
        self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name)

        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
        else:
            self.device = torch.device("cpu")

        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        self.sentiment_model.to(self.device)
        self.sentiment_model.eval()
        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–∞ –≥–æ—Ç–æ–≤–∞!")

    def __call__(
        self,
        train_path: str,
        train_path_news: str,
        test_path: str,
        test_path_news: str,
        output_path: str
    ):
        # === 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
        train_df = pd.read_csv(train_path)
        train_df.index = pd.to_datetime(train_df['begin'])

        test_df = pd.read_csv(test_path)
        test_df.index = pd.to_datetime(test_df['begin'])

        full_df = pd.concat([train_df, test_df], ignore_index=True)
        full_df = full_df.sort_values(['ticker', 'begin'])
        full_df.index = pd.to_datetime(full_df['begin'])
        full_df.drop(columns='begin', inplace=True)

        news_train = pd.read_csv(train_path_news)
        if 'Unnamed: 0' in news_train.columns:
            news_train.drop(columns=['Unnamed: 0'], inplace=True)
        news_train.index = pd.to_datetime(news_train['publish_date'])
        news_train.drop(columns=['publish_date'], inplace=True)
        news_train.index.rename('begin', inplace=True)

        news_test = pd.read_csv(test_path_news)
        news_test.index = pd.to_datetime(news_test['publish_date'])
        news_test.drop(columns=['publish_date'], inplace=True)
        news_test.index.rename('begin', inplace=True)

        news_full_df = pd.concat([news_train, news_test])

        # === 2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ===
        COMPANY_TO_TICKER = {
            # === –ë–∞–Ω–∫–∏ ===
            "—Å–±–µ—Ä–±–∞–Ω–∫": "SBER", "—Å–±–µ—Ä": "SBER", "—Å–±–µ—Ä–±–∞–Ω–∫ —Ä–æ—Å—Å–∏–∏": "SBER", "–ø–∞–æ —Å–±–µ—Ä–±–∞–Ω–∫": "SBER",
            "sberbank": "SBER", "sber": "SBER",
            "–≤—Ç–±": "VTBR", "–±–∞–Ω–∫ –≤—Ç–±": "VTBR", "–ø–∞–æ –≤—Ç–±": "VTBR", "vtb": "VTBR",
            "–≥–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫": "GPB", "–≥–ø–±": "GPB", "gazprombank": "GPB",
            "–∞–ª—å—Ñ–∞-–±–∞–Ω–∫": "ALBK", "–∞–ª—å—Ñ–∞–±–∞–Ω–∫": "ALBK", "alfabank": "ALBK",
            "—Ä–æ—Å–±–∞–Ω–∫": "ROSB", "rosbank": "ROSB",
            "–æ—Ç–∫—Ä—ã—Ç–∏–µ": "OPMD", "–±–∞–Ω–∫ –æ—Ç–∫—Ä—ã—Ç–∏–µ": "OPMD", "—Ñ–∫ –æ—Ç–∫—Ä—ã—Ç–∏–µ": "OPMD",
            "—Å–±–µ—Ä–±–∞–Ω–∫ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ": "SGZH", "—Å–≥–∂": "SGZH", "sgzh": "SGZH",

            # === –≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞ –∏ –Ω–µ—Ñ—Ç—å ===
            "–≥–∞–∑–ø—Ä–æ–º": "GAZP", "–ø–∞–æ –≥–∞–∑–ø—Ä–æ–º": "GAZP", "gazprom": "GAZP",
            "—Ä–æ—Å–Ω–µ—Ñ—Ç—å": "ROSN", "–ø–∞–æ —Ä–æ—Å–Ω–µ—Ñ—Ç—å": "ROSN", "rosneft": "ROSN",
            "–ª—É–∫–æ–π–ª": "LKOH", "lukoil": "LKOH",
            "–Ω–æ–≤–∞—Ç—ç–∫": "NVTK", "–ø–∞–æ –Ω–æ–≤–∞—Ç—ç–∫": "NVTK", "novatek": "NVTK",
            "—Å—É—Ä–≥—É—Ç–Ω–µ—Ñ—Ç–µ–≥–∞–∑": "SNGS", "—Å–Ω–≥": "SNGS", "surgutneftegas": "SNGS",
            "—Ç–∞—Ç–Ω–µ—Ñ—Ç—å": "TATN", "–æ–∞–æ —Ç–∞—Ç–Ω–µ—Ñ—Ç—å": "TATN", "tatneft": "TATN",
            "—Ç—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å": "TRNF", "transneft": "TRNF",
            "–∏–Ω—Ç–µ—Ä —Ä–∞–æ": "IRAO", "–∏–Ω—Ç–µ—Ä—Ä–∞–æ": "IRAO", "inter rao": "IRAO", "inter-rao": "IRAO",
            "–º–æ—Å—ç–Ω–µ—Ä–≥–æ": "MSNG", "msk —ç–Ω–µ—Ä–≥–æ": "MSNG", "mosenergo": "MSNG",
            "—Ä—É—Å–≥–∏–¥—Ä–æ": "HYDR", "rusgidro": "HYDR",
            "—Ñ—Å–∫ –µ—ç—Å": "FEES", "—Ä–æ—Å—Å–µ—Ç–∏": "RSTI", "rosseti": "RSTI",

            # === –ú–µ—Ç–∞–ª–ª—ã –∏ –≥–æ—Ä–Ω–æ–¥–æ–±—ã—á–∞ ===
            "–Ω–æ—Ä–Ω–∏–∫–µ–ª—å": "GMKN", "–Ω–æ—Ä–∏–ª—å—Å–∫–∏–π –Ω–∏–∫–µ–ª—å": "GMKN", "mmk": "MAGN",
            "–º–º–∫": "MAGN", "–º–∞–≥–Ω–∏—Ç–æ–≥–æ—Ä—Å–∫–∏–π –º–µ—Ç–∞–ª–ª—É—Ä–≥–∏—á–µ—Å–∫–∏–π –∫–æ–º–±–∏–Ω–∞—Ç": "MAGN",
            "—Å–µ–≤–µ—Ä—Å—Ç–∞–ª—å": "CHMF", "severstal": "CHMF",
            "–Ω–ª–º–∫": "NLMK", "–Ω–æ–≤–æ–ª–∏–ø–µ—Ü–∫–∏–π –º–µ—Ç–∞–ª–ª—É—Ä–≥–∏—á–µ—Å–∫–∏–π –∫–æ–º–±–∏–Ω–∞—Ç": "NLMK",
            "–µ–≤—Ä–∞–∑": "EVRG", "evraz": "EVRG",
            "–ø–æ–ª—é—Å": "PLZL", "polyus": "PLZL",
            "–∞–ª—Ä–æ—Å–∞": "ALRS", "alrosa": "ALRS",
            "—É—Ä–∞–ª–∫–∞–ª–∏–π": "URKA", "uralkali": "URKA",
            "—Ñ–æ—Å–∞–≥—Ä–æ": "PHOR", "phosagro": "PHOR",
            "–º–µ—á–µ–ª": "MTLR", "mechel": "MTLR",

            # === –¢–µ–ª–µ–∫–æ–º –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ ===
            "–º—Ç—Å": "MTSS", "mts": "MTSS", "–ø–∞–æ –º—Ç—Å": "MTSS",
            "–±–∏–ª–∞–π–Ω": "VEON", "–≤—ã–º–ø–µ–ª–∫–æ–º": "VEON", "beeline": "VEON",
            "–º–µ–≥–∞—Ñ–æ–Ω": "MFON", "megafon": "MFON",
            "—è–Ω–¥–µ–∫—Å": "YNDX", "yandex": "YNDX",
            "vk": "VKCO", "–≤–∫": "VKCO", "–≤–∫–æ–Ω—Ç–∞–∫—Ç–µ": "VKCO", "mail.ru": "VKCO",
            "ozon": "OZON", "–æ–∑–æ–Ω": "OZON",
            "wildberries": "WB", "–≤–∞–π–ª–¥–±–µ—Ä—Ä–∏–∑": "WB", "–≤–±": "WB",
            "–¥–∑–µ–Ω": "YNDX",  # —á–∞—Å—Ç—å –Ø–Ω–¥–µ–∫—Å–∞

            # === –†–∏—Ç–µ–π–ª –∏ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏–π —Å–µ–∫—Ç–æ—Ä ===
            "–º–∞–≥–Ω–∏—Ç": "MGNT", "magnit": "MGNT",
            "–ª–µ–Ω—Ç–∞": "LNTA", "lenta": "LNTA",
            "—Ö5 —Ä–∏—Ç–µ–π–ª": "FIVE", "–ø—è—Ç—ë—Ä–æ—á–∫–∞": "FIVE", "–ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–æ–∫": "FIVE", "x5 retail": "FIVE",
            "–≥–ª–æ–±—É—Å": "GLTR", "globus": "GLTR",
            "–¥–∏–∫—Å–∏": "DIXY", "dixy": "DIXY",
            "—ç–ª—å–¥–æ—Ä–∞–¥–æ": "SALM", "eldorado": "SALM",  # —á–µ—Ä–µ–∑ –°–∞–ª—ã–º
            "—Å–±–µ—Ä–º–∞—Ä–∫–µ—Ç": "SBER",  # —á–∞—Å—Ç—å –°–±–µ—Ä–∞
            "–∞–≤–∏—Ç–æ": "YNDX",  # –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –Ø–Ω–¥–µ–∫—Å—É

            # === –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∏ –ª–æ–≥–∏—Å—Ç–∏–∫–∞ ===
            "–∞—ç—Ä–æ—Ñ–ª–æ—Ç": "AFLT", "aeroflot": "AFLT",
            "—Å–±–µ—Ä–ª–æ–≥–∏—Å—Ç–∏–∫–∞": "SBER",  # –ø–æ–∫–∞ –Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ç–∏–∫–µ—Ä
            "–¥–µ–ª—å—Ç–∞": "TGKA", "delta": "TGKA",  # –¢–ì–ö-1
            "–≥–ª–æ–±–∞–ª—Ç—Ä–∞–Ω—Å": "GLTR",  # –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ: –∫–æ–Ω—Ñ–ª–∏–∫—Ç —Å Globus (—Ä–∏—Ç–µ–π–ª), –Ω–æ —Ç–∏–∫–µ—Ä –¥—Ä—É–≥–æ–π
            "–Ω–º–∞": "NMTP", "–Ω–º—Ç–ø": "NMTP", "–Ω–æ–≤–æ—Ä–æ—Å—Å–∏–π—Å–∫–∏–π –º–æ—Ä—Å–∫–æ–π —Ç–æ—Ä–≥–æ–≤—ã–π –ø–æ—Ä—Ç": "NMTP",

            # === –§–∏–Ω–∞–Ω—Å—ã –∏ –±–∏—Ä–∂–∏ ===
            "–º–æ—Å–±–∏—Ä–∂–∞": "MOEX", "–º–æ—Å–∫–æ–≤—Å–∫–∞—è –±–∏—Ä–∂–∞": "MOEX", "moex": "MOEX",
            "—Å–ø–± –±–∏—Ä–∂–∞": "SPBE", "spb exchange": "SPBE", "—Å–ø–±exchange": "SPBE",
            "–∞—Ñ–∫ —Å–∏—Å—Ç–µ–º–∞": "AFKS", "—Å–∏—Å—Ç–µ–º–∞": "AFKS", "afk sistema": "AFKS",
            "—Å–∞–º–æ–ª–µ—Ç": "SMLT", "samolet": "SMLT",
            "–ø–∏–æ–Ω–µ—Ä": "PIKK", "pik": "PIKK",

            # === –•–æ–ª–¥–∏–Ω–≥–∏ –∏ –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã ===
            "—Å–µ–≤–µ—Ä–≥—Ä—É–ø–ø": "SVCB",  # –°–µ–≤–µ—Ä—Å—Ç–∞–ª—å-–≥—Ä—É–ø–ø
            "—Ä–æ—Å–Ω–µ—Ñ—Ç–µ–≥–∞–∑": "–†–ù–ì",  # –Ω–µ —Ç–æ—Ä–≥—É–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ

            # === –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –∏ —Ç—Ä–∞–Ω—Å–ª–∏—Ç-–≤–∞—Ä–∏–∞–Ω—Ç—ã ===
            "gazprom neft": "SIBN", "–≥–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å": "SIBN", "—Å—É—Ä–≥—É—Ç–Ω–µ—Ñ—Ç—å": "SNGS",
            "tatneft": "TATN", "lukoil": "LKOH", "novatek": "NVTK",
            "polyus gold": "PLZL", "alrosa": "ALRS", "phosagro": "PHOR",
            "severstal": "CHMF", "nlmk": "NLMK", "evraz": "EVRG",
            "mts russia": "MTSS", "veon": "VEON", "megafon": "MFON",
            "ozon holdings": "OZON", "wild berries": "WB",
            "yandex nv": "YNDX", "vk group": "VKCO",
            "magnit retail": "MGNT", "lenta group": "LNTA",
            "x5 group": "FIVE", "aeroflot group": "AFLT",
        }

        ALLOWED_TICKERS_SET = set(COMPANY_TO_TICKER.values())
        MACRO_KEYWORDS = [
            "–∏–Ω—Ñ–ª—è—Ü–∏—è", "–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞", "—Ü–±", "–±–∞–Ω–∫ —Ä–æ—Å—Å–∏–∏", "–≤–≤–ø", "–±–µ–∑—Ä–∞–±–æ—Ç–∏—Ü–∞",
            "–∫—É—Ä—Å —Ä—É–±–ª—è", "–¥–µ—Ñ–∏—Ü–∏—Ç", "–±—é–¥–∂–µ—Ç", "–∏–º–ø–æ—Ä—Ç", "—ç–∫—Å–ø–æ—Ä—Ç", "–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å",
            "—Ä—ã–Ω–æ–∫ —Ç—Ä—É–¥–∞", "–ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç—å", "–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏–µ —Ü–µ–Ω—ã", "–∏–ø—Ü"
        ]

        # === 3. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===
        def normalize_text(text: str) -> str:
            text = text.lower()
            text = re.sub(r"[^\w\s]", " ", text)
            text = re.sub(r"\s+", " ", text).strip()
            return text

        def detect_ticker(title: str, text: str) -> str:
            content = normalize_text(f"{title} {text}")
            for variant, ticker in COMPANY_TO_TICKER.items():
                if variant in content:
                    return ticker
            return "none"

        def detect_macro(title: str, text: str) -> bool:
            content = normalize_text(f"{title} {text}")
            return any(kw in content for kw in MACRO_KEYWORDS)

        def get_sentiment_batch(texts, batch_size=256):
            labels = ["negative", "neutral", "positive"]
            results = []
            for i in tqdm(range(0, len(texts), batch_size), desc="–°–µ–Ω—Ç–∏–º–µ–Ω—Ç"):
                batch = texts[i:i + batch_size]
                inputs = self.sentiment_tokenizer(
                    batch,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=256
                ).to(self.device)
                with torch.no_grad():
                    logits = self.sentiment_model(**inputs).logits
                preds = torch.argmax(logits, dim=-1)
                results.extend([labels[p] for p in preds.cpu().numpy()])
            return results

        # === 4. –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π ===
        print("üîç –ê–Ω–∞–ª–∏–∑ —Ç–∏–∫–µ—Ä–æ–≤ –∏ –º–∞–∫—Ä–æ...")
        news_analyze = news_full_df.copy()
        news_analyze['ticker'] = [
            detect_ticker(row.title, row.publication)
            for row in tqdm(news_analyze.itertuples(), total=len(news_analyze), desc="–¢–∏–∫–µ—Ä—ã")
        ]

        news_analyze['is_macro'] = [
            detect_macro(row.title, row.publication)
            for row in tqdm(news_analyze.itertuples(), total=len(news_analyze), desc="–ú–∞–∫—Ä–æ")
        ]

        print("üß† –ê–Ω–∞–ª–∏–∑ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–∞...")
        news_analyze['sentiment'] = get_sentiment_batch(
            [f"{row.title}. {row.publication}" for row in news_analyze.itertuples()],
            batch_size=256
        )

        news_analyze.index = news_analyze.index - pd.Timedelta(days=1)
        news_df = news_analyze.copy()
        news_df.index = pd.to_datetime(news_df.index)

        sentiment_map = {'positive': 1, 'negative': -1, 'neutral': 0}
        news_df['sentiment_encoded'] = news_df['sentiment'].map(sentiment_map)
        news_df['is_macro_encoded'] = np.where(news_df['is_macro'], news_df['sentiment_encoded'], 0)
        news_df['date'] = news_df.index.date

        news_agg = news_df.groupby(['date', 'ticker']).agg({
            'sentiment_encoded': 'mean',
            'is_macro_encoded': 'mean'
        }).reset_index()
        
        prices_df = full_df.reset_index()
        prices_df['begin'] = pd.to_datetime(prices_df['begin']).dt.date

        merged_df = prices_df.merge(
            news_agg,
            left_on=['begin', 'ticker'],
            right_on=['date', 'ticker'],
            how='left'
        )

        merged_df['sentiment_encoded'] = merged_df['sentiment_encoded'].fillna(0)
        merged_df['is_macro_encoded'] = merged_df['is_macro_encoded'].fillna(0)
        merged_df = merged_df.drop(columns=['date'])
        merged_df.set_index('begin', inplace=True)
        merged_df.index = pd.to_datetime(merged_df.index)

        full_df_clean = merged_df.reset_index().drop_duplicates(subset=['begin', 'ticker'], keep='last').set_index('begin')
        df_pivot = full_df_clean.set_index('ticker', append=True).unstack('ticker')
        df_pivot.ffill(inplace=True)

        close = df_pivot['close']
        X = generate_features(df_pivot).stack().sort_index()
        X.dropna(inplace=True)

        # === 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ª–µ–π–±–ª–æ–≤ –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ ===
        y_dict = {}
        vol_dict = {}

        close.index = pd.to_datetime(close.index)

        for period in range(1, self.max_period+1):
            print(f"Generating labels and vol for period = {period}")
            tripple_barrier_label = close.apply(
                three_barrier_std,
                ptSl=[1, 1],
                rolling_n=50,
                base_scaling_factor=2.0,
                base_period=10,
                period=period
            )
            y_raw = tripple_barrier_label.stack()
            y = y_raw.reindex(X.index).dropna()
            y = y + 1  # -1 ‚Üí 0, 0 ‚Üí 1, 1 ‚Üí 2
            y_dict[period] = y

            barrier = close.apply(
                barrier_std,
                ptSl=[1, 1],
                rolling_n=50,
                base_scaling_factor=2.0,
                base_period=10,
                period=period
            )
            vol_raw = barrier.stack()
            vol = vol_raw.reindex(X.index).dropna()
            vol_dict[period] = vol

        y_df = pd.DataFrame({f'{p}': y for p, y in y_dict.items()})
        vol_df = pd.DataFrame({f'{p}': v for p, v in vol_dict.items()})

        train_end = train_df.index[-1]
        test_start = test_df.index[0]
        X_train = X[:train_end]
        X_test = X[test_start:]
        y_train = y_df[:train_end]
        y_test = y_df[test_start:]
        vol_train = vol_df[:train_end]
        vol_test = vol_df[test_start:]

        # === 6. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ ===
        sample_weights = class_weight.compute_sample_weight("balanced", y_train['1'])

        def objective(trial):
            params = {
                'objective': 'multi:softmax',
                'num_class': 3,
                'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                'max_depth': trial.suggest_int('max_depth', 3, 12),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
                'gamma': trial.suggest_float('gamma', 0, 0.5),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 1),
                'random_state': 42,
                'eval_metric': 'mlogloss'
            }

            tscv = TimeSeriesSplit(n_splits=5)
            scores = []
            for train_idx, val_idx in tscv.split(X_train):
                X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
                y_tr, y_val = y_train.iloc[train_idx]['1'], y_train.iloc[val_idx]['1']
                fold_sample_weights = class_weight.compute_sample_weight("balanced", y_tr)
                model = XGBClassifier(**params)
                model.fit(X_tr, y_tr, sample_weight=fold_sample_weights)
                y_pred = model.predict(X_val)
                score = balanced_accuracy_score(y_val, y_pred)
                scores.append(score)
            return np.mean(scores)

        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))
        study.optimize(objective, n_trials=50, show_progress_bar=True)

        best_params = study.best_params
        best_model_base = XGBClassifier(
            objective='multi:softmax',
            num_class=3,
            random_state=42,
            eval_metric='mlogloss',
            **best_params
        )

        # === 7. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –≤—Å–µ—Ö –ø–µ—Ä–∏–æ–¥–æ–≤ ===
        index_common = X_test.index
        pred_classes = pd.DataFrame(index=index_common)
        pred_proba_all = pd.DataFrame(index=index_common)

        for period in range(1, self.max_period+1):
            print(f"Training on period = {period}")
            y_tr = y_train[f'{period}']
            y_te = y_test[f'{period}']

            X_tr_aligned = X_train.reindex(y_tr.index)
            weights = class_weight.compute_sample_weight("balanced", y_tr)

            model = deepcopy(best_model_base)
            model.fit(X_tr_aligned, y_tr, sample_weight=weights)

            X_te_aligned = X_test.reindex(y_te.index)
            y_pred = model.predict(X_te_aligned)
            y_pred_proba = model.predict_proba(X_te_aligned)

            pred_classes[str(period)] = pd.Series(y_pred, index=y_te.index)
            pred_proba_all[f'prob_{period}_class_0'] = pd.Series(y_pred_proba[:, 0], index=y_te.index)
            pred_proba_all[f'prob_{period}_class_1'] = pd.Series(y_pred_proba[:, 1], index=y_te.index)
            pred_proba_all[f'prob_{period}_class_2'] = pd.Series(y_pred_proba[:, 2], index=y_te.index)

        pred_classes = pred_classes.reindex(index_common)
        pred_proba_all = pred_proba_all.reindex(index_common)

        pred_returns = y_test.copy()
        for period in range(1, self.max_period+1):
            pred_returns[f'{period}'] = np.where(
                pred_classes[f'{period}'] == 2,
                vol_test[f'{period}'] * pred_proba_all[f'prob_{period}_class_2'],
                np.where(
                    pred_classes[f'{period}'] == 0,
                    -vol_test[f'{period}'] * pred_proba_all[f'prob_{period}_class_0'],
                    vol_test[f'{period}'] * (pred_proba_all[f'prob_{period}_class_2'] - pred_proba_all[f'prob_{period}_class_0'])
                )
            )

        last_date = pred_returns.index.get_level_values('begin').max()
        sub_df = pred_returns.xs(last_date, level='begin')
        sub_df.to_csv(output_path)
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {output_path}")